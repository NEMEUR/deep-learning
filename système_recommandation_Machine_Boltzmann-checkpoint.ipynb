{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#librairies\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn #construire des ANN\n",
    "import torch.nn.parallel #pour les calculs parallèles\n",
    "import torch.optim as optim # pour l'optimiser\n",
    "import torch.utils.data\n",
    "from torch.autograd import variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Toy Story (1995)</td>\n",
       "      <td>Animation|Children's|Comedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Jumanji (1995)</td>\n",
       "      <td>Adventure|Children's|Fantasy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Grumpier Old Men (1995)</td>\n",
       "      <td>Comedy|Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Waiting to Exhale (1995)</td>\n",
       "      <td>Comedy|Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Father of the Bride Part II (1995)</td>\n",
       "      <td>Comedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3878</th>\n",
       "      <td>3948</td>\n",
       "      <td>Meet the Parents (2000)</td>\n",
       "      <td>Comedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3879</th>\n",
       "      <td>3949</td>\n",
       "      <td>Requiem for a Dream (2000)</td>\n",
       "      <td>Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3880</th>\n",
       "      <td>3950</td>\n",
       "      <td>Tigerland (2000)</td>\n",
       "      <td>Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3881</th>\n",
       "      <td>3951</td>\n",
       "      <td>Two Family House (2000)</td>\n",
       "      <td>Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3882</th>\n",
       "      <td>3952</td>\n",
       "      <td>Contender, The (2000)</td>\n",
       "      <td>Drama|Thriller</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3883 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0                                   1                             2\n",
       "0        1                    Toy Story (1995)   Animation|Children's|Comedy\n",
       "1        2                      Jumanji (1995)  Adventure|Children's|Fantasy\n",
       "2        3             Grumpier Old Men (1995)                Comedy|Romance\n",
       "3        4            Waiting to Exhale (1995)                  Comedy|Drama\n",
       "4        5  Father of the Bride Part II (1995)                        Comedy\n",
       "...    ...                                 ...                           ...\n",
       "3878  3948             Meet the Parents (2000)                        Comedy\n",
       "3879  3949          Requiem for a Dream (2000)                         Drama\n",
       "3880  3950                    Tigerland (2000)                         Drama\n",
       "3881  3951             Two Family House (2000)                         Drama\n",
       "3882  3952               Contender, The (2000)                Drama|Thriller\n",
       "\n",
       "[3883 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#importer le jeu de données\n",
    "movies=pd.read_csv(\"Downloads/ml-1m/movies.dat\",sep=\"::\",header=None, engine=\"python\",encoding='latin-1')\n",
    "movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>48067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>M</td>\n",
       "      <td>56</td>\n",
       "      <td>16</td>\n",
       "      <td>70072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>M</td>\n",
       "      <td>25</td>\n",
       "      <td>15</td>\n",
       "      <td>55117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>M</td>\n",
       "      <td>45</td>\n",
       "      <td>7</td>\n",
       "      <td>02460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>M</td>\n",
       "      <td>25</td>\n",
       "      <td>20</td>\n",
       "      <td>55455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6035</th>\n",
       "      <td>6036</td>\n",
       "      <td>F</td>\n",
       "      <td>25</td>\n",
       "      <td>15</td>\n",
       "      <td>32603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6036</th>\n",
       "      <td>6037</td>\n",
       "      <td>F</td>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "      <td>76006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6037</th>\n",
       "      <td>6038</td>\n",
       "      <td>F</td>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>14706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6038</th>\n",
       "      <td>6039</td>\n",
       "      <td>F</td>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "      <td>01060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6039</th>\n",
       "      <td>6040</td>\n",
       "      <td>M</td>\n",
       "      <td>25</td>\n",
       "      <td>6</td>\n",
       "      <td>11106</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6040 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0  1   2   3      4\n",
       "0        1  F   1  10  48067\n",
       "1        2  M  56  16  70072\n",
       "2        3  M  25  15  55117\n",
       "3        4  M  45   7  02460\n",
       "4        5  M  25  20  55455\n",
       "...    ... ..  ..  ..    ...\n",
       "6035  6036  F  25  15  32603\n",
       "6036  6037  F  45   1  76006\n",
       "6037  6038  F  56   1  14706\n",
       "6038  6039  F  45   0  01060\n",
       "6039  6040  M  25   6  11106\n",
       "\n",
       "[6040 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users=pd.read_csv(\"Downloads/ml-1m/users.dat\",sep=\"::\",header=None, engine=\"python\",encoding='latin-1')\n",
    "users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1193</td>\n",
       "      <td>5</td>\n",
       "      <td>978300760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>661</td>\n",
       "      <td>3</td>\n",
       "      <td>978302109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>914</td>\n",
       "      <td>3</td>\n",
       "      <td>978301968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3408</td>\n",
       "      <td>4</td>\n",
       "      <td>978300275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2355</td>\n",
       "      <td>5</td>\n",
       "      <td>978824291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000204</th>\n",
       "      <td>6040</td>\n",
       "      <td>1091</td>\n",
       "      <td>1</td>\n",
       "      <td>956716541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000205</th>\n",
       "      <td>6040</td>\n",
       "      <td>1094</td>\n",
       "      <td>5</td>\n",
       "      <td>956704887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000206</th>\n",
       "      <td>6040</td>\n",
       "      <td>562</td>\n",
       "      <td>5</td>\n",
       "      <td>956704746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000207</th>\n",
       "      <td>6040</td>\n",
       "      <td>1096</td>\n",
       "      <td>4</td>\n",
       "      <td>956715648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000208</th>\n",
       "      <td>6040</td>\n",
       "      <td>1097</td>\n",
       "      <td>4</td>\n",
       "      <td>956715569</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000209 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0     1  2          3\n",
       "0           1  1193  5  978300760\n",
       "1           1   661  3  978302109\n",
       "2           1   914  3  978301968\n",
       "3           1  3408  4  978300275\n",
       "4           1  2355  5  978824291\n",
       "...       ...   ... ..        ...\n",
       "1000204  6040  1091  1  956716541\n",
       "1000205  6040  1094  5  956704887\n",
       "1000206  6040   562  5  956704746\n",
       "1000207  6040  1096  4  956715648\n",
       "1000208  6040  1097  4  956715569\n",
       "\n",
       "[1000209 rows x 4 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings=pd.read_csv(\"Downloads/ml-1m/ratings.dat\",sep=\"::\",header=None, engine=\"python\",encoding='latin-1')\n",
    "ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>874965758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>876893171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>878542960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>876893119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>889751712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79995</th>\n",
       "      <td>943</td>\n",
       "      <td>1067</td>\n",
       "      <td>2</td>\n",
       "      <td>875501756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79996</th>\n",
       "      <td>943</td>\n",
       "      <td>1074</td>\n",
       "      <td>4</td>\n",
       "      <td>888640250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79997</th>\n",
       "      <td>943</td>\n",
       "      <td>1188</td>\n",
       "      <td>3</td>\n",
       "      <td>888640250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79998</th>\n",
       "      <td>943</td>\n",
       "      <td>1228</td>\n",
       "      <td>3</td>\n",
       "      <td>888640275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79999</th>\n",
       "      <td>943</td>\n",
       "      <td>1330</td>\n",
       "      <td>3</td>\n",
       "      <td>888692465</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>80000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0     1  2          3\n",
       "0        1     1  5  874965758\n",
       "1        1     2  3  876893171\n",
       "2        1     3  4  878542960\n",
       "3        1     4  3  876893119\n",
       "4        1     5  3  889751712\n",
       "...    ...   ... ..        ...\n",
       "79995  943  1067  2  875501756\n",
       "79996  943  1074  4  888640250\n",
       "79997  943  1188  3  888640250\n",
       "79998  943  1228  3  888640275\n",
       "79999  943  1330  3  888692465\n",
       "\n",
       "[80000 rows x 4 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#prépapartion du jeu d'entrainement et du jeu de test\n",
    "training_set=pd.read_csv(\"Downloads/ml-100k/u1.base\",sep=\"\\t\",header=None)\n",
    "training_set   #dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set=np.array(training_set,dtype='int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>887431973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>875693118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>878542960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>5</td>\n",
       "      <td>874965706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "      <td>875073198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19995</th>\n",
       "      <td>458</td>\n",
       "      <td>648</td>\n",
       "      <td>4</td>\n",
       "      <td>886395899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19996</th>\n",
       "      <td>458</td>\n",
       "      <td>1101</td>\n",
       "      <td>4</td>\n",
       "      <td>886397931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19997</th>\n",
       "      <td>459</td>\n",
       "      <td>934</td>\n",
       "      <td>3</td>\n",
       "      <td>879563639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19998</th>\n",
       "      <td>460</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>882912371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19999</th>\n",
       "      <td>462</td>\n",
       "      <td>682</td>\n",
       "      <td>5</td>\n",
       "      <td>886365231</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0     1  2          3\n",
       "0        1     6  5  887431973\n",
       "1        1    10  3  875693118\n",
       "2        1    12  5  878542960\n",
       "3        1    14  5  874965706\n",
       "4        1    17  3  875073198\n",
       "...    ...   ... ..        ...\n",
       "19995  458   648  4  886395899\n",
       "19996  458  1101  4  886397931\n",
       "19997  459   934  3  879563639\n",
       "19998  460    10  3  882912371\n",
       "19999  462   682  5  886365231\n",
       "\n",
       "[20000 rows x 4 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set=pd.read_csv(\"Downloads/ml-100k/u1.test\",sep=\"\\t\",header=None)\n",
    "test_set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set=np.array(test_set,dtype='int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "943 1682\n"
     ]
    }
   ],
   "source": [
    "#récupérer le nombre des utilisateurs et le nombre de films\n",
    "nb_users=int(max(max(training_set[:,0]),max(test_set[:,0])))\n",
    "nb_movies=int(max(max(training_set[:,1]),max(test_set[:,1])))\n",
    "print(nb_users, nb_movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convertir les données en matrices \n",
    "def convert(data):\n",
    "    new_data = []\n",
    "    for id_users in range(1, nb_users + 1):\n",
    "        id_movies = data[:,1][data[:,0] == id_users]#data[data[:,1]==id_users,1]  #récupérer tous les films notés par l'id_user  \n",
    "        id_ratings =data[:,2][data[:,0] == id_users] #data[[data[:,2]==id_users,2]  \n",
    "        rating = np.zeros(nb_movies)\n",
    "        rating[id_movies - 1] = id_ratings\n",
    "        new_data.append(list(rating))\n",
    "    return new_data\n",
    "training_set = convert(training_set)\n",
    "test_set = convert(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convertir les données en tensors (matrices) pour pytorch\n",
    "training_set=torch.FloatTensor(training_set)\n",
    "test_set=torch.FloatTensor(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prédiction binaire= proposer un système de recommandation pour voir \n",
    "#convertir les notes en notes binaires 1 si un utilisateur a aimé un film ; 0 s'il n'a pas aimé \n",
    "training_set[training_set==0]=-1\n",
    "training_set[training_set==1]=0\n",
    "training_set[training_set==2]=0\n",
    "training_set[training_set>=3]=1\n",
    "\n",
    "test_set[test_set==0]=-1\n",
    "test_set[test_set==1]=0\n",
    "test_set[test_set==2]=0\n",
    "test_set[test_set>=3]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "nv=len(training_set[0])#nb de colonnes(nb_movies)\n",
    "nh=100\n",
    "batch_size=100\n",
    "rbm = RBM(nv, nh) #création de notre rbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#créer l'architecture de la machine boltzmann\n",
    "class RBM():\n",
    "    def __init__(self, nv, nh): #initialisation des poids et des biais (a,b)\n",
    "        self.W = torch.randn(nh, nv)\n",
    "        self.a = torch.randn(1, nh)\n",
    "        self.b = torch.randn(1, nv)\n",
    "    def sample_h(self, x):#echantillonnage des noeuds hidden\n",
    "        wx = torch.mm(x, self.W.t()) #produit matriciel\n",
    "        activation = wx + self.a.expand_as(wx)\n",
    "        p_h_given_v = torch.sigmoid(activation)\n",
    "        return p_h_given_v, torch.bernoulli(p_h_given_v)#on utilise la loi de bernoulli pour générer l'echantillonnage de gibbs\n",
    "    def sample_v(self, y):\n",
    "        wy = torch.mm(y, self.W)\n",
    "        activation = wy + self.b.expand_as(wy)\n",
    "        p_v_given_h = torch.sigmoid(activation)\n",
    "        return p_v_given_h, torch.bernoulli(p_v_given_h)\n",
    "    def train(self, v0, vk, ph0, phk):#vi: valeur des neurones visibles à l'état i, phi=probabilité des neuronnes cachées=1 sachant le neurone visible est l'état i\n",
    "        #mettre à jour les poids après k itérations de l'échantillonnage\n",
    "         self.W += torch.mm(ph0, v0) - torch.mm(phk, vk)\n",
    "        self.b += torch.sum((v0 - vk), 0)\n",
    "        self.a += torch.sum((ph0 - phk), 0)\n",
    "nv = len(training_set[0])\n",
    "nh = 100\n",
    "batch_size = 100\n",
    "rbm = RBM(nv, nh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 100, 200, 300, 400, 500, 600, 700, 800]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(range(0,nb_users-batch_size,batch_size))#batch_size= un pas de 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 loss: tensor(0.3126)\n",
      "epoch: 2 loss: tensor(0.2460)\n",
      "epoch: 3 loss: tensor(0.2524)\n",
      "epoch: 4 loss: tensor(0.2508)\n",
      "epoch: 5 loss: tensor(0.2516)\n",
      "epoch: 6 loss: tensor(0.2486)\n",
      "epoch: 7 loss: tensor(0.2533)\n",
      "epoch: 8 loss: tensor(0.2496)\n",
      "epoch: 9 loss: tensor(0.2501)\n",
      "epoch: 10 loss: tensor(0.2515)\n"
     ]
    }
   ],
   "source": [
    "# entrainement de RBM\n",
    "nb_epoch = 10\n",
    "for epoch in range(1, nb_epoch + 1):\n",
    "    train_loss = 0#initialisation de la fonction de coût ()\n",
    "    cpt = 0.#compteur pour normaliser l'erreur\n",
    "    for id_user in range(0, nb_users - batch_size, batch_size):\n",
    "        v0 = training_set[id_user:id_user+batch_size] #(0-99)\n",
    "        vk = training_set[id_user:id_user+batch_size]\n",
    "        ph0,_ = rbm.sample_h(v0)#probabilité d'obtenir les neurones cachés\n",
    "        for k in range(10):#reconstruire l'echantillange de Gibbs k fois\n",
    "            _,hk = rbm.sample_h(vk)#construire les noeuds cachées suivant les noeuds visibles\n",
    "            _,vk = rbm.sample_v(hk)#puis constreuire les noeuds visibiles à partir des noeuds cachées \n",
    "            vk[v0<0] = v0[v0<0] #ne pas mettre à jour les valeurs =-1 lorsque l'utilisateur n'a pas donné de notes sur un film\n",
    "        phk,_ = rbm.sample_h(vk) #probabilité d'obtenir les neurones visibles\n",
    "        rbm.train(v0, vk, ph0, phk)\n",
    "        train_loss += torch.mean(torch.abs(v0[v0>=0] - vk[v0>=0]))#calcul de l\"erreur la distance absolue moyenn\n",
    "        cpt += 1.\n",
    "    print('epoch: '+str(epoch)+' loss: '+str(train_loss/cpt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " loss: tensor(0.2117)\n",
      " loss: tensor(0.1513)\n",
      " loss: tensor(0.2291)\n",
      " loss: tensor(0.2468)\n",
      " loss: tensor(0.2689)\n",
      " loss: tensor(0.2538)\n",
      " loss: tensor(0.2408)\n",
      " loss: tensor(0.2280)\n",
      " loss: tensor(0.2026)\n",
      " loss: tensor(0.1935)\n",
      " loss: tensor(0.2058)\n",
      " loss: tensor(0.1983)\n",
      " loss: tensor(0.2082)\n",
      " loss: tensor(0.2058)\n",
      " loss: tensor(0.2254)\n",
      " loss: tensor(0.2193)\n",
      " loss: tensor(0.2260)\n",
      " loss: tensor(0.2191)\n",
      " loss: tensor(0.2128)\n",
      " loss: tensor(0.2181)\n",
      " loss: tensor(0.2247)\n",
      " loss: tensor(0.2239)\n",
      " loss: tensor(0.2197)\n",
      " loss: tensor(0.2136)\n",
      " loss: tensor(0.2072)\n",
      " loss: tensor(0.2086)\n",
      " loss: tensor(0.2207)\n",
      " loss: tensor(0.2181)\n",
      " loss: tensor(0.2187)\n",
      " loss: tensor(0.2170)\n",
      " loss: tensor(0.2150)\n",
      " loss: tensor(0.2198)\n",
      " loss: tensor(0.2192)\n",
      " loss: tensor(0.2186)\n",
      " loss: tensor(0.2195)\n",
      " loss: tensor(0.2210)\n",
      " loss: tensor(0.2204)\n",
      " loss: tensor(0.2223)\n",
      " loss: tensor(0.2223)\n",
      " loss: tensor(0.2206)\n",
      " loss: tensor(0.2210)\n",
      " loss: tensor(0.2218)\n",
      " loss: tensor(0.2224)\n",
      " loss: tensor(0.2215)\n",
      " loss: tensor(0.2212)\n",
      " loss: tensor(0.2178)\n",
      " loss: tensor(0.2132)\n",
      " loss: tensor(0.2125)\n",
      " loss: tensor(0.2174)\n",
      " loss: tensor(0.2204)\n",
      " loss: tensor(0.2216)\n",
      " loss: tensor(0.2210)\n",
      " loss: tensor(0.2216)\n",
      " loss: tensor(0.2238)\n",
      " loss: tensor(0.2280)\n",
      " loss: tensor(0.2283)\n",
      " loss: tensor(0.2293)\n",
      " loss: tensor(0.2272)\n",
      " loss: tensor(0.2282)\n",
      " loss: tensor(0.2264)\n",
      " loss: tensor(0.2300)\n",
      " loss: tensor(0.2313)\n",
      " loss: tensor(0.2319)\n",
      " loss: tensor(0.2313)\n",
      " loss: tensor(0.2316)\n",
      " loss: tensor(0.2308)\n",
      " loss: tensor(0.2315)\n",
      " loss: tensor(0.2345)\n",
      " loss: tensor(0.2327)\n",
      " loss: tensor(0.2328)\n",
      " loss: tensor(0.2313)\n",
      " loss: tensor(0.2299)\n",
      " loss: tensor(0.2276)\n",
      " loss: tensor(0.2245)\n",
      " loss: tensor(0.2276)\n",
      " loss: tensor(0.2281)\n",
      " loss: tensor(0.2272)\n",
      " loss: tensor(0.2261)\n",
      " loss: tensor(0.2250)\n",
      " loss: tensor(0.2255)\n",
      " loss: tensor(0.2280)\n",
      " loss: tensor(0.2287)\n",
      " loss: tensor(0.2306)\n",
      " loss: tensor(0.2293)\n",
      " loss: tensor(0.2291)\n",
      " loss: tensor(0.2303)\n",
      " loss: tensor(0.2301)\n",
      " loss: tensor(0.2309)\n",
      " loss: tensor(0.2311)\n",
      " loss: tensor(0.2300)\n",
      " loss: tensor(0.2298)\n",
      " loss: tensor(0.2301)\n",
      " loss: tensor(0.2296)\n",
      " loss: tensor(0.2297)\n",
      " loss: tensor(0.2299)\n",
      " loss: tensor(0.2286)\n",
      " loss: tensor(0.2281)\n",
      " loss: tensor(0.2276)\n",
      " loss: tensor(0.2281)\n",
      " loss: tensor(0.2286)\n",
      " loss: tensor(0.2301)\n",
      " loss: tensor(0.2319)\n",
      " loss: tensor(0.2310)\n",
      " loss: tensor(0.2326)\n",
      " loss: tensor(0.2318)\n",
      " loss: tensor(0.2306)\n",
      " loss: tensor(0.2306)\n",
      " loss: tensor(0.2296)\n",
      " loss: tensor(0.2303)\n",
      " loss: tensor(0.2313)\n",
      " loss: tensor(0.2314)\n",
      " loss: tensor(0.2314)\n",
      " loss: tensor(0.2329)\n",
      " loss: tensor(0.2321)\n",
      " loss: tensor(0.2323)\n",
      " loss: tensor(0.2336)\n",
      " loss: tensor(0.2330)\n",
      " loss: tensor(0.2321)\n",
      " loss: tensor(0.2319)\n",
      " loss: tensor(0.2321)\n",
      " loss: tensor(0.2328)\n",
      " loss: tensor(0.2336)\n",
      " loss: tensor(0.2324)\n",
      " loss: tensor(0.2329)\n",
      " loss: tensor(0.2337)\n",
      " loss: tensor(0.2347)\n",
      " loss: tensor(0.2361)\n",
      " loss: tensor(0.2352)\n",
      " loss: tensor(0.2376)\n",
      " loss: tensor(0.2375)\n",
      " loss: tensor(0.2362)\n",
      " loss: tensor(0.2344)\n",
      " loss: tensor(0.2346)\n",
      " loss: tensor(0.2351)\n",
      " loss: tensor(0.2351)\n",
      " loss: tensor(0.2338)\n",
      " loss: tensor(0.2334)\n",
      " loss: tensor(0.2329)\n",
      " loss: tensor(0.2320)\n",
      " loss: tensor(0.2323)\n",
      " loss: tensor(0.2329)\n",
      " loss: tensor(0.2334)\n",
      " loss: tensor(0.2318)\n",
      " loss: tensor(0.2314)\n",
      " loss: tensor(0.2320)\n",
      " loss: tensor(0.2304)\n",
      " loss: tensor(0.2296)\n",
      " loss: tensor(0.2297)\n",
      " loss: tensor(0.2324)\n",
      " loss: tensor(0.2324)\n",
      " loss: tensor(0.2320)\n",
      " loss: tensor(0.2320)\n",
      " loss: tensor(0.2324)\n",
      " loss: tensor(0.2312)\n",
      " loss: tensor(0.2326)\n",
      " loss: tensor(0.2318)\n",
      " loss: tensor(0.2319)\n",
      " loss: tensor(0.2316)\n",
      " loss: tensor(0.2327)\n",
      " loss: tensor(0.2332)\n",
      " loss: tensor(0.2343)\n",
      " loss: tensor(0.2340)\n",
      " loss: tensor(0.2326)\n",
      " loss: tensor(0.2328)\n",
      " loss: tensor(0.2319)\n",
      " loss: tensor(0.2341)\n",
      " loss: tensor(0.2348)\n",
      " loss: tensor(0.2357)\n",
      " loss: tensor(0.2347)\n",
      " loss: tensor(0.2367)\n",
      " loss: tensor(0.2366)\n",
      " loss: tensor(0.2362)\n",
      " loss: tensor(0.2363)\n",
      " loss: tensor(0.2365)\n",
      " loss: tensor(0.2363)\n",
      " loss: tensor(0.2357)\n",
      " loss: tensor(0.2355)\n",
      " loss: tensor(0.2353)\n",
      " loss: tensor(0.2363)\n",
      " loss: tensor(0.2367)\n",
      " loss: tensor(0.2387)\n",
      " loss: tensor(0.2398)\n",
      " loss: tensor(0.2397)\n",
      " loss: tensor(0.2394)\n",
      " loss: tensor(0.2385)\n",
      " loss: tensor(0.2390)\n",
      " loss: tensor(0.2379)\n",
      " loss: tensor(0.2379)\n",
      " loss: tensor(0.2377)\n",
      " loss: tensor(0.2374)\n",
      " loss: tensor(0.2374)\n",
      " loss: tensor(0.2379)\n",
      " loss: tensor(0.2377)\n",
      " loss: tensor(0.2385)\n",
      " loss: tensor(0.2388)\n",
      " loss: tensor(0.2390)\n",
      " loss: tensor(0.2397)\n",
      " loss: tensor(0.2396)\n",
      " loss: tensor(0.2410)\n",
      " loss: tensor(0.2409)\n",
      " loss: tensor(0.2414)\n",
      " loss: tensor(0.2423)\n",
      " loss: tensor(0.2423)\n",
      " loss: tensor(0.2426)\n",
      " loss: tensor(0.2422)\n",
      " loss: tensor(0.2434)\n",
      " loss: tensor(0.2434)\n",
      " loss: tensor(0.2431)\n",
      " loss: tensor(0.2434)\n",
      " loss: tensor(0.2430)\n",
      " loss: tensor(0.2431)\n",
      " loss: tensor(0.2419)\n",
      " loss: tensor(0.2414)\n",
      " loss: tensor(0.2410)\n",
      " loss: tensor(0.2408)\n",
      " loss: tensor(0.2404)\n",
      " loss: tensor(0.2415)\n",
      " loss: tensor(0.2404)\n",
      " loss: tensor(0.2408)\n",
      " loss: tensor(0.2409)\n",
      " loss: tensor(0.2410)\n",
      " loss: tensor(0.2412)\n",
      " loss: tensor(0.2417)\n",
      " loss: tensor(0.2425)\n",
      " loss: tensor(0.2433)\n",
      " loss: tensor(0.2427)\n",
      " loss: tensor(0.2426)\n",
      " loss: tensor(0.2434)\n",
      " loss: tensor(0.2453)\n",
      " loss: tensor(0.2452)\n",
      " loss: tensor(0.2450)\n",
      " loss: tensor(0.2445)\n",
      " loss: tensor(0.2440)\n",
      " loss: tensor(0.2442)\n",
      " loss: tensor(0.2435)\n",
      " loss: tensor(0.2437)\n",
      " loss: tensor(0.2430)\n",
      " loss: tensor(0.2423)\n",
      " loss: tensor(0.2418)\n",
      " loss: tensor(0.2418)\n",
      " loss: tensor(0.2423)\n",
      " loss: tensor(0.2436)\n",
      " loss: tensor(0.2432)\n",
      " loss: tensor(0.2435)\n",
      " loss: tensor(0.2438)\n",
      " loss: tensor(0.2448)\n",
      " loss: tensor(0.2438)\n",
      " loss: tensor(0.2439)\n",
      " loss: tensor(0.2438)\n",
      " loss: tensor(0.2438)\n",
      " loss: tensor(0.2433)\n",
      " loss: tensor(0.2428)\n",
      " loss: tensor(0.2423)\n",
      " loss: tensor(0.2424)\n",
      " loss: tensor(0.2433)\n",
      " loss: tensor(0.2434)\n",
      " loss: tensor(0.2429)\n",
      " loss: tensor(0.2430)\n",
      " loss: tensor(0.2424)\n",
      " loss: tensor(0.2434)\n",
      " loss: tensor(0.2434)\n",
      " loss: tensor(0.2437)\n",
      " loss: tensor(0.2434)\n",
      " loss: tensor(0.2432)\n",
      " loss: tensor(0.2432)\n",
      " loss: tensor(0.2429)\n",
      " loss: tensor(0.2426)\n",
      " loss: tensor(0.2428)\n",
      " loss: tensor(0.2432)\n",
      " loss: tensor(0.2429)\n",
      " loss: tensor(0.2429)\n",
      " loss: tensor(0.2422)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2413)\n",
      " loss: tensor(0.2410)\n",
      " loss: tensor(0.2410)\n",
      " loss: tensor(0.2410)\n",
      " loss: tensor(0.2405)\n",
      " loss: tensor(0.2409)\n",
      " loss: tensor(0.2411)\n",
      " loss: tensor(0.2422)\n",
      " loss: tensor(0.2425)\n",
      " loss: tensor(0.2422)\n",
      " loss: tensor(0.2417)\n",
      " loss: tensor(0.2415)\n",
      " loss: tensor(0.2417)\n",
      " loss: tensor(0.2413)\n",
      " loss: tensor(0.2409)\n",
      " loss: tensor(0.2411)\n",
      " loss: tensor(0.2413)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2410)\n",
      " loss: tensor(0.2414)\n",
      " loss: tensor(0.2414)\n",
      " loss: tensor(0.2414)\n",
      " loss: tensor(0.2415)\n",
      " loss: tensor(0.2415)\n",
      " loss: tensor(0.2414)\n",
      " loss: tensor(0.2413)\n",
      " loss: tensor(0.2413)\n",
      " loss: tensor(0.2413)\n",
      " loss: tensor(0.2417)\n",
      " loss: tensor(0.2420)\n",
      " loss: tensor(0.2420)\n",
      " loss: tensor(0.2421)\n",
      " loss: tensor(0.2419)\n",
      " loss: tensor(0.2417)\n",
      " loss: tensor(0.2414)\n",
      " loss: tensor(0.2417)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2415)\n",
      " loss: tensor(0.2412)\n",
      " loss: tensor(0.2414)\n",
      " loss: tensor(0.2415)\n",
      " loss: tensor(0.2409)\n",
      " loss: tensor(0.2407)\n",
      " loss: tensor(0.2406)\n",
      " loss: tensor(0.2409)\n",
      " loss: tensor(0.2415)\n",
      " loss: tensor(0.2417)\n",
      " loss: tensor(0.2414)\n",
      " loss: tensor(0.2411)\n",
      " loss: tensor(0.2410)\n",
      " loss: tensor(0.2407)\n",
      " loss: tensor(0.2410)\n",
      " loss: tensor(0.2408)\n",
      " loss: tensor(0.2410)\n",
      " loss: tensor(0.2409)\n",
      " loss: tensor(0.2407)\n",
      " loss: tensor(0.2404)\n",
      " loss: tensor(0.2403)\n",
      " loss: tensor(0.2403)\n",
      " loss: tensor(0.2403)\n",
      " loss: tensor(0.2403)\n",
      " loss: tensor(0.2406)\n",
      " loss: tensor(0.2409)\n",
      " loss: tensor(0.2411)\n",
      " loss: tensor(0.2407)\n",
      " loss: tensor(0.2404)\n",
      " loss: tensor(0.2403)\n",
      " loss: tensor(0.2409)\n",
      " loss: tensor(0.2412)\n",
      " loss: tensor(0.2410)\n",
      " loss: tensor(0.2408)\n",
      " loss: tensor(0.2408)\n",
      " loss: tensor(0.2410)\n",
      " loss: tensor(0.2409)\n",
      " loss: tensor(0.2415)\n",
      " loss: tensor(0.2428)\n",
      " loss: tensor(0.2423)\n",
      " loss: tensor(0.2422)\n",
      " loss: tensor(0.2424)\n",
      " loss: tensor(0.2417)\n",
      " loss: tensor(0.2415)\n",
      " loss: tensor(0.2413)\n",
      " loss: tensor(0.2415)\n",
      " loss: tensor(0.2414)\n",
      " loss: tensor(0.2415)\n",
      " loss: tensor(0.2410)\n",
      " loss: tensor(0.2409)\n",
      " loss: tensor(0.2406)\n",
      " loss: tensor(0.2407)\n",
      " loss: tensor(0.2410)\n",
      " loss: tensor(0.2417)\n",
      " loss: tensor(0.2419)\n",
      " loss: tensor(0.2419)\n",
      " loss: tensor(0.2415)\n",
      " loss: tensor(0.2421)\n",
      " loss: tensor(0.2419)\n",
      " loss: tensor(0.2419)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2419)\n",
      " loss: tensor(0.2417)\n",
      " loss: tensor(0.2421)\n",
      " loss: tensor(0.2423)\n",
      " loss: tensor(0.2423)\n",
      " loss: tensor(0.2421)\n",
      " loss: tensor(0.2421)\n",
      " loss: tensor(0.2420)\n",
      " loss: tensor(0.2419)\n",
      " loss: tensor(0.2418)\n",
      " loss: tensor(0.2424)\n",
      " loss: tensor(0.2421)\n",
      " loss: tensor(0.2419)\n",
      " loss: tensor(0.2423)\n",
      " loss: tensor(0.2427)\n",
      " loss: tensor(0.2426)\n",
      " loss: tensor(0.2423)\n",
      " loss: tensor(0.2420)\n",
      " loss: tensor(0.2423)\n",
      " loss: tensor(0.2422)\n",
      " loss: tensor(0.2422)\n",
      " loss: tensor(0.2424)\n",
      " loss: tensor(0.2425)\n",
      " loss: tensor(0.2421)\n",
      " loss: tensor(0.2420)\n",
      " loss: tensor(0.2421)\n",
      " loss: tensor(0.2417)\n",
      " loss: tensor(0.2418)\n",
      " loss: tensor(0.2412)\n",
      " loss: tensor(0.2414)\n",
      " loss: tensor(0.2410)\n",
      " loss: tensor(0.2404)\n",
      " loss: tensor(0.2410)\n",
      " loss: tensor(0.2418)\n",
      " loss: tensor(0.2417)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2417)\n",
      " loss: tensor(0.2421)\n",
      " loss: tensor(0.2418)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2417)\n",
      " loss: tensor(0.2420)\n",
      " loss: tensor(0.2414)\n",
      " loss: tensor(0.2415)\n",
      " loss: tensor(0.2414)\n",
      " loss: tensor(0.2418)\n",
      " loss: tensor(0.2420)\n",
      " loss: tensor(0.2414)\n",
      " loss: tensor(0.2412)\n",
      " loss: tensor(0.2409)\n",
      " loss: tensor(0.2409)\n",
      " loss: tensor(0.2415)\n",
      " loss: tensor(0.2419)\n",
      " loss: tensor(0.2415)\n",
      " loss: tensor(0.2413)\n",
      " loss: tensor(0.2414)\n",
      " loss: tensor(0.2415)\n",
      " loss: tensor(0.2415)\n",
      " loss: tensor(0.2410)\n",
      " loss: tensor(0.2410)\n",
      " loss: tensor(0.2411)\n",
      " loss: tensor(0.2420)\n",
      " loss: tensor(0.2422)\n",
      " loss: tensor(0.2423)\n",
      " loss: tensor(0.2423)\n",
      " loss: tensor(0.2423)\n",
      " loss: tensor(0.2417)\n",
      " loss: tensor(0.2419)\n",
      " loss: tensor(0.2419)\n",
      " loss: tensor(0.2423)\n",
      " loss: tensor(0.2423)\n",
      " loss: tensor(0.2425)\n",
      " loss: tensor(0.2437)\n",
      " loss: tensor(0.2432)\n",
      " loss: tensor(0.2432)\n",
      " loss: tensor(0.2426)\n",
      " loss: tensor(0.2424)\n",
      " loss: tensor(0.2424)\n",
      " loss: tensor(0.2431)\n",
      " loss: tensor(0.2432)\n",
      " loss: tensor(0.2436)\n",
      " loss: tensor(0.2439)\n",
      " loss: tensor(0.2434)\n",
      " loss: tensor(0.2434)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " loss: tensor(0.2437)\n",
      " loss: tensor(0.2431)\n",
      " loss: tensor(0.2426)\n",
      " loss: tensor(0.2421)\n",
      " loss: tensor(0.2421)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n",
      " loss: tensor(0.2416)\n"
     ]
    }
   ],
   "source": [
    "#test de la rbm\n",
    "test_loss = 0\n",
    "cpt = 0.\n",
    "for id_user in range(nb_users):\n",
    "    v = training_set[id_user:id_user+1] \n",
    "    vt= test_set[id_user:id_user+1] #target\n",
    "    if len(vt[vt>=0])>0:\n",
    "        _,h = rbm.sample_h(v)\n",
    "        _,v= rbm.sample_v(h)\n",
    "        test_loss += torch.mean(torch.abs(vt[vt>=0] - v[vt>=0]))#calcul l'erreur là où il ya des prédictions à faire\n",
    "        cpt += 1.\n",
    "    print(' loss: '+str(test_loss/cpt))#  0,25 correspond à 75 % de succès."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
